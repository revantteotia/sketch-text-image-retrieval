{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "from torchvision import transforms\n",
    "\n",
    "def show_pil_img(img, title=None):\n",
    "    \"\"\"\n",
    "    Gets PIL image as input and plots it\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.imshow(img)\n",
    "\n",
    "def show_normalized_image_tensor(img_tensor, mean = (0.48145466, 0.4578275, 0.40821073),\n",
    "                                 std = (0.26862954, 0.26130258, 0.27577711),\n",
    "                                 title=None):\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean=[-0.485/0.229, -0.4578275/0.224, -0.40821073/0.255],\n",
    "        std=[1/0.26862954, 1/0.26130258, 1/0.27577711]\n",
    "        )\n",
    "    inv_tensor = inv_normalize(img_tensor)\n",
    "    npimg = inv_tensor.numpy()\n",
    "    plt.figure()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from dataloader.stir_dataset import STIRDataset\n",
    "from dataloader.data_loaders import get_dataloader\n",
    "\n",
    "from model import composition_models\n",
    "from utils.simple_tokenizer import SimpleTokenizer\n",
    "from utils.util import set_seed, mkdir, load_config_file, write_json\n",
    "from utils.logger import setup_logger\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_CONFIG_PATH = 'configs/dataset_config.yaml'\n",
    "TRAINER_CONFIG_PATH = 'configs/train_config.yaml'\n",
    "\n",
    "data_config = load_config_file(DATA_CONFIG_PATH)\n",
    "train_config = load_config_file(TRAINER_CONFIG_PATH)\n",
    "\n",
    "config = OmegaConf.merge(train_config, data_config)\n",
    "\n",
    "config.device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting text tokenizer\n",
    "tokenizer = SimpleTokenizer()\n",
    "\n",
    "# getting dataset for validation\n",
    "val_dataset = STIRDataset(data_config, tokenizer, split='val')\n",
    "train_dataset = STIRDataset(data_config, tokenizer, split='train')\n",
    "\n",
    "# getting model\n",
    "texts = train_dataset.get_all_texts()\n",
    "config.model = 'concat'\n",
    "config.embed_dim = 512\n",
    "config.n_gpu = 1\n",
    "opt = config\n",
    "\"\"\"Builds the model and related optimizer.\"\"\"\n",
    "print('Creating model and optimizer for', opt.model)\n",
    "if opt.model == 'concat':\n",
    "    model = composition_models.Concat(texts, embed_dim=opt.embed_dim)\n",
    "\n",
    "checkpoint_path = \"/home/trevant/DL4CV/project_saved_checkpoints/checkpoint_best_concat.pt\"\n",
    "assert checkpoint_path is not None\n",
    "assert os.path.isfile(checkpoint_path)\n",
    "\n",
    "print(f\"Loading saved checkpoint at {checkpoint_path}\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(torch.device(config.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ranking all targets for all queries\n",
    "model.eval()\n",
    "losses = []\n",
    "all_query_features = []\n",
    "all_target_features = []\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "# testing for first 1000 data points only\n",
    "indices = torch.randperm(len(val_dataset))[:1000]\n",
    "small_set = torch.utils.data.Subset(val_dataset, indices)\n",
    "\n",
    "eval_dataloader = get_dataloader(config, small_set, is_train=True)\n",
    "with torch.no_grad():\n",
    "    for step, batch in tqdm(enumerate(eval_dataloader), desc=\"evaluating\"):\n",
    "        query_img_input, query_text_input, target_img_input = batch['query_img_input'], batch['query_text'], batch['target_img_input']\n",
    "\n",
    "        query_img_input = query_img_input.to(torch.device(config.device))\n",
    "        # query_text_input = query_text_input.to(torch.device(config.device))\n",
    "        target_img_input = target_img_input.to(torch.device(config.device))\n",
    "        \n",
    "        # FWD dunction itself calculates loss\n",
    "        composition_features = model.compose_img_text(query_img_input, query_text_input)\n",
    "        target_image_features = model.extract_img_feature(target_img_input)\n",
    "        \n",
    "        all_query_features.append(composition_features)\n",
    "        all_target_features.append(target_image_features)\n",
    "\n",
    "    all_query_features = torch.vstack(all_query_features)\n",
    "    all_target_features = torch.vstack(all_target_features)\n",
    "\n",
    "    # normalizing\n",
    "    # normalized features\n",
    "    all_query_features = all_query_features / all_query_features.norm(dim=-1, keepdim=True)\n",
    "    all_target_features = all_target_features / all_target_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = all_query_features @ all_target_features.t()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_targets = torch.argsort(similarity, dim=1, descending=True)\n",
    "print(\"sorted targets\", sorted_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualizing the retrieved images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = \"/home/trevant/DL4CV/project_datasets/VG_ALL\" # path of VG images\n",
    "SKETCHES_PATH = \"/home/trevant/DL4CV/project_datasets/imagenet-sketch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_TO_VIEW = 42\n",
    "\n",
    "query_img_path = val_dataset[INDEX_TO_VIEW]['query_img_path']\n",
    "query_text = val_dataset[INDEX_TO_VIEW]['query_text']\n",
    "gt_target = val_dataset[INDEX_TO_VIEW]['target_img_id']\n",
    "\n",
    "top_retrieved_images_paths = []\n",
    "for target_idx in sorted_targets[INDEX_TO_VIEW, :10]:\n",
    "    target_img_id = val_dataset[target_idx]['target_img_id']\n",
    "    top_retrieved_images_paths.append(target_img_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Directly sketch reading path from datset\n",
    "query_img_path = os.path.join(SKETCHES_PATH, query_img_path)\n",
    "query_img = Image.open(query_img_path)\n",
    "show_pil_img(query_img, title=f\"+ {query_text}\")\n",
    "# ----------\n",
    "show_normalized_image_tensor(val_dataset[INDEX_TO_VIEW]['target_img_input'], \n",
    "                             title=f\"GT TARGET\")\n",
    "\n",
    "for i, target_idx in enumerate(sorted_targets[INDEX_TO_VIEW, :10]):\n",
    "    show_normalized_image_tensor(val_dataset[target_idx]['target_img_input'], title=f\"rank {i+1}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33b0a4d85c84bd7e7dbb4e0d99d3e96fe18b7ea9ea87e065888161c4ad29f37c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('dright': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
